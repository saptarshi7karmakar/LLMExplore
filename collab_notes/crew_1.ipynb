{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP+KY+kkeX6RoZM4DDIe4O0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saptarshi7karmakar/LLMExplore/blob/main/collab_notes/crew_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGfy7mH5sbqZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --q crewai\n",
        "!pip install --q -U duckduckgo-search\n",
        "!pip install --q langchain_google_genai\n",
        "!pip install --q langchain-community\n",
        "!pip install --q crewai-tools"
      ],
      "metadata": {
        "id": "xOj-BuKARJrH",
        "collapsed": true
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --q crewai-tools"
      ],
      "metadata": {
        "id": "95IHpLFkzCgi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, LLM\n",
        "from google.colab import userdata\n",
        "from litellm import completion\n",
        "\n",
        "# Switched to LiteLM Standards to ensure Tools Compatibility\n",
        "# gemini-pro\n",
        "# Gemini 2.0 Flash-Lite\n",
        "\n",
        "llm_chat = LLM(\n",
        "    api_key=userdata.get('GOOGLE_API_KEY'),\n",
        "    model=\"gemini/gemini-pro\",\n",
        ")\n",
        "llm_chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7YOqWB-HL4i",
        "outputId": "104a5feb-eb22-4fcb-f2f0-b3505e13ef28"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<crewai.llm.LLM at 0x7ea3211eb150>"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chat.get_context_window_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiomBnDCXirw",
        "outputId": "a996546d-bff8-43a2-b372-2dc7d55418c5"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6963"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from google.colab import userdata\n",
        "# from litellm import completion\n",
        "\n",
        "# # os.environ['GEMINI_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# mod = \"gemini/gemini-pro\"\n",
        "# # mod = \"google/gemma-3-1b-it\"\n",
        "# # completion(model=\"gemini/gemini-pro\")\n",
        "# llm_chat = ChatGoogleGenerativeAI(model=my_llm,\n",
        "#                                   verbose = True,\n",
        "#                                   temperature = 0.6,\n",
        "#                                   google_api_key = userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "5nNQYoFQsqBf"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAuMV92tGKJY",
        "outputId": "85ebc8c5-0921-4ae8-83ef-1ec84c3235f8"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<crewai.llm.LLM at 0x7ea3211eb150>"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid\n",
        "from crewai import Agent, Task, Crew, Process"
      ],
      "metadata": {
        "id": "J9U8qs3aRLfh"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Search and Analyze Use Cases"
      ],
      "metadata": {
        "id": "251A9PjVT6s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Search Tool from Base Tools to workaround the dependency issue\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from crewai.tools import BaseTool\n",
        "\n",
        "class MyCustomDuckDuckGoTool(BaseTool):\n",
        "    name: str = \"DuckDuckGo Search Tool\"\n",
        "    description: str = \"Search the web for a given query.\"\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        # Ensure the DuckDuckGoSearchRun is invoked properly.\n",
        "        duckduckgo_tool = DuckDuckGoSearchRun()\n",
        "        response = duckduckgo_tool.invoke(query)\n",
        "        return response\n",
        "\n",
        "    def _get_tool(self):\n",
        "        # Create an instance of the tool when needed\n",
        "        return MyCustomDuckDuckGoTool()\n",
        "\n",
        "search_tool = MyCustomDuckDuckGoTool()\n",
        "search_tool_for_evaluator = MyCustomDuckDuckGoTool()"
      ],
      "metadata": {
        "id": "sVBW1UnKuuWn"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "research_agent = Agent(\n",
        "                       name = \"Research Agent\",\n",
        "                       role = \"Senior Researcher\",\n",
        "                       goal = \"Uncover most useful parameters in Open Table Formats\",\n",
        "                       backstory = \"\"\"You work at a leading tech company.\n",
        "                       Your expertise is in identifying all influencial parameters from the industruy for technologies in Data Intensive Engineering Space.\n",
        "                       You have a knack for dissecting complex data and presenting\"\"\",\n",
        "                       verbose=True,\n",
        "                       allow_delegation=False,\n",
        "                       tools = [search_tool],\n",
        "                       llm = llm_chat)\n"
      ],
      "metadata": {
        "id": "-jAOVSSISABN"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_agent = Agent(\n",
        "                       name = \"Analysis & Decision Agent\",\n",
        "                       role = \"Senior Analyst & Decision Maker\",\n",
        "                       goal = \"Analyze and Craft Recommendation with compelling reasoning\",\n",
        "                       backstory = \"\"\"\"You are a Industry trend analyst, known for depth of analysis and data driven recommendations.\n",
        "                       you analyze variety of aspects of the product to come up with compelling recommendations\"\"\",\n",
        "                       verbose=True,\n",
        "                       allow_delegation=False,\n",
        "                       tools = [],\n",
        "                       llm = llm_chat\n",
        ")\n",
        "\n",
        "evaluator_agent = Agent(\n",
        "                       name = \"Recommender Agent\",\n",
        "                       role = \"Leader\",\n",
        "                       goal = \"Understand, Review,  Cleanse the Information to craft final recommendation\",\n",
        "                       backstory = \"\"\"\"You are the senior technology leader, who is expert in converging to final decision based on options.\n",
        "                       You review different proposals to curve out most impactful decision based on current org and industry understanding\"\"\",\n",
        "                       verbose=True,\n",
        "                       allow_delegation=False,\n",
        "                       tools = [search_tool_for_evaluator],\n",
        "                       llm = llm_chat\n",
        ")"
      ],
      "metadata": {
        "id": "y9ELWjnk6b_y"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tasks for the agents\n",
        "\n",
        "task1 = Task(\n",
        "  description=\"\"\"Conduct a comprehensive analysis of the latest advancements in Opentable Formats in the industry such as Delta Lake, Iceberg and others.\n",
        "  Identify key parameters, applications on on premises and cloud set up, and potential industry future directions.\n",
        "  Your final answer MUST be a table with all collected parameters and possible data points\"\"\",\n",
        "  expected_output=\"\"\"A detailed table summarizing all key parameters.\n",
        "  The structure of table will have each rows representing the value of the deciding parameter for each option.\"\"\",\n",
        "  agent=research_agent\n",
        ")\n",
        "\n",
        "task2 = Task(\n",
        "    description=\"\"\"Using the information provided in the table format, run deep research and analysis to come up with recommendation with suitable ranking.\n",
        "    the analysis should suggest ranking for the use cases where all the systems for the target use cases are on premises infrastructure.\n",
        "    the final report should contain top 5 picks with deep down data points and decisioning parameters and visualisations on the considered data.\n",
        "    \"\"\",\n",
        "    expected_output=\"A report providing ranked recommendations for Open Table Formats in on-premises environments, including data points and decisioning parameters.\",\n",
        "    agent=decision_agent\n",
        ")\n",
        "\n",
        "task3 = Task(\n",
        "    description=\"\"\"\n",
        "    Review the final information provided with ranks, recommendations.\n",
        "    Deep dive into the options to tally against the industry trends and current organisational need to build a robust on premises solution.\n",
        "    Cleanse all the information to depict inly the relevant recommendation.\"\"\",\n",
        "    expected_output=\"A crisp well structured report with top relevant options with reasoning.\",\n",
        "    agent=evaluator_agent\n",
        ")"
      ],
      "metadata": {
        "id": "FP_2pAyFvlP6"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crew definition and in action.\n",
        "opentable_crew = Crew(\n",
        "    name=\"OpenTable Crew\",\n",
        "    agents=[research_agent, decision_agent, evaluator_agent],\n",
        "    tasks=[task1, task2, task3],\n",
        "    verbose=True,\n",
        "    process=Process.sequential,\n",
        "    max_rpm=15 # Introduced to not rate limit for the entire crew\n",
        ")\n",
        "opentable_crew"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h06hxc__qmO",
        "outputId": "69e45dce-1427-44f0-a813-e3c53e391890"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Crew(id=02b89fe7-f186-451b-9818-08e756526b05, process=Process.sequential, number_of_agents=2, number_of_tasks=2)"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opentable_crew.check_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82EHTkiPXyGY",
        "outputId": "16f6da6f-0f51-4e1a-8711-2f64bbb89381"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Crew(id=02b89fe7-f186-451b-9818-08e756526b05, process=Process.sequential, number_of_agents=2, number_of_tasks=2)"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opentable_crew.kickoff()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w306M7GdAWoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Io_RIHpCAiFY"
      },
      "execution_count": 104,
      "outputs": []
    }
  ]
}